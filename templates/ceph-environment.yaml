parameter_defaults:
  CephConfigOverrides:
    mon_max_pg_per_osd: 3072
    journal_size: 5120
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    osd_pool_default_pg_num: 16
    osd_pool_default_pgp_num: 16
    osd_recovery_op_priority: 2
    osd_recovery_max_active: 2
    osd_max_backfills: 1
  CephAnsibleDisksConfig:
    dmcrypt: true
    osd_scenario: lvm
    osd_objectstore: bluestore
    devices:
      - /dev/sda
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
  CephPools:
    - {"name": default.rgw.buckets.data, "pg_num": 256, "pgp_num": 256, "application": rados}
    - {"name": default.rgw.buckets.index, "pg_num": 32, "pgp_num": 32, "application": rados}
    - {"name": images, "pg_num": 64, "pgp_num": 64, "application": rbd}
    - {"name": metrics, "pg_num": 128, "pgp_num": 128, "application":openstack_gnocchi}
    - {"name": backups, "pg_num": 128, "pgp_num": 128, "application": rbd}
    - {"name": vms, "pg_num": 128, "pgp_num": 128, "application": rbd}
    - {"name": volumes, "pg_num": 512, "pgp_num": 512, "application": rbd}
